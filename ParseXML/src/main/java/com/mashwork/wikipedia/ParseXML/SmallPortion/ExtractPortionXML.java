package com.mashwork.wikipedia.ParseXML.SmallPortion;
//import wikipediaParser.*;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Iterator;


//import org.codehaus.stax2.XMLOutputFactory2;
//import org.codehaus.stax2.*;

/**
 * @author  Jiali Huang
 *			Computer Science Department, 
 *			Courant Institute Mathematical Sciences, NYU
 * @time	
 * Given a list of names, this class can extract a small portion of dump from the whole wikidump.
 * The list of names can be generated by <code>CategoryPortionExtraction.java</code> or
 * <code>ExtractTitles.java</code>.
 */
public class ExtractPortionXML {
	
	public static void main(String[] args) throws IOException
	{
      if (args.length < 5) {
      System.out.println("USAGE: ExtractLinks <TitleFileDir> <WholeDumpDir> <outputXMLDir> <pageNotFound> <DumpTotalPage>");
      System.out.println("<DumpTotalPage> for wikidump2013-06-04 is 13539091");
      System.exit(255);
  }

//		String titleDir = "/Users/Ricky/mashwork/wikiXmlParser/crawledXML/new/Category2010sTvTitle.txt";
//		String XMLDir = "/Users/Ricky/mashwork/wikidump/new/enwiki-20130604-pages-articles.xml";
//		String output = "/Users/Ricky/mashwork/wikiXmlParser/crawledXML/new/Category2010sTvPages.xml";	
//		String titleNotFound = "/Users/Ricky/mashwork/wikiXmlParser/crawledXML/new/Category2010sTvTitlesNotFound.xml";
		
      	String titleDir = args[0];
      	String XMLDir = args[1];
      	String output = args[2];
      	String titleNotFound = args[3];
		int dumpTotalPage = Integer.parseInt(args[4]);
      	
		FileWriter FW = new FileWriter(titleNotFound);
		
		//HashSet<String> list = loadTitles(titleDir);
		HashMap<String,Integer> list = loadTitles(titleDir);
        System.out.println("The number of pages to extract: "+list.size());
		
		
        PortionExtractor portionExtractor = new PortionExtractor(XMLDir,list,output,dumpTotalPage);
        portionExtractor.parse();
        
        
        Iterator<String> it = portionExtractor.toCrawl.keySet().iterator();
        {
        	while(it.hasNext())
        	{
        		String query = it.next();
        		if(portionExtractor.toCrawl.get(query)==0)
        		{
        			FW.write(query+"\n");
        		}
        	}
        }
        FW.close();
	}
	
	public static HashMap<String,Integer> loadTitles(String dir) throws IOException
	{
		FileInputStream FS = new FileInputStream(dir);
		InputStreamReader SR = new InputStreamReader(FS);
		BufferedReader BR = new BufferedReader(SR);
		String line;
		HashMap<String,Integer> titles = new HashMap<String,Integer>();
		//HashSet<String> titles = new HashSet<String>();
		while((line = BR.readLine())!=null)
		{
			titles.put(line,0);
		}
		BR.close();
		return titles;
	}
}
